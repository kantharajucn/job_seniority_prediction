{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SmartRecruiters.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "11d69aa2-4acb-4cbd-9656-9d50eeb078cc"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6db40fba6bf34537a6cc9fb95214cf1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_022b7935ced14edb93a161192764dfef",
              "IPY_MODEL_9b0f00e0faf54b598c0306661db50463"
            ],
            "layout": "IPY_MODEL_d929ce6f0eb242a1b79ac1960621aa26"
          }
        },
        "022b7935ced14edb93a161192764dfef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62235fb123a14f7daeb21909fbbc9889",
            "max": 213450,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19b586901676423db50fce5769bac21b",
            "value": 213450
          }
        },
        "9b0f00e0faf54b598c0306661db50463": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27e949fae5d5431f852d6abfa4c47665",
            "placeholder": "​",
            "style": "IPY_MODEL_15591c42effd499a88af3c17cbdd0402",
            "value": " 213k/213k [00:00&lt;00:00, 622kB/s]"
          }
        },
        "d929ce6f0eb242a1b79ac1960621aa26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62235fb123a14f7daeb21909fbbc9889": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19b586901676423db50fce5769bac21b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "27e949fae5d5431f852d6abfa4c47665": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15591c42effd499a88af3c17cbdd0402": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "456de311b2db4a15abfd7f7e104bc844": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a19733716464e969b2deb31f3cdc6f7",
              "IPY_MODEL_0322df96ffe04f50a01d09f112941bca"
            ],
            "layout": "IPY_MODEL_e85da681564c454c908132a1f99dcf50"
          }
        },
        "7a19733716464e969b2deb31f3cdc6f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89d4242fc76d4531bc47bd258f436ba1",
            "max": 29,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b395c3104f634c9b8a5144dd38ed58bc",
            "value": 29
          }
        },
        "0322df96ffe04f50a01d09f112941bca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f60f6e1427444f7aef63b8b36878149",
            "placeholder": "​",
            "style": "IPY_MODEL_270d32dd22794e71b9192257dc7744e9",
            "value": " 29.0/29.0 [00:00&lt;00:00, 285B/s]"
          }
        },
        "e85da681564c454c908132a1f99dcf50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89d4242fc76d4531bc47bd258f436ba1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b395c3104f634c9b8a5144dd38ed58bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "5f60f6e1427444f7aef63b8b36878149": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "270d32dd22794e71b9192257dc7744e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0290aeb843704674a555176f05590bf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0deeecb2ef7d40029e962a71e23c0c96",
              "IPY_MODEL_c8d7ed7323a941028b07a47cb6f96378"
            ],
            "layout": "IPY_MODEL_4127176475e44814b9a4904af8571f44"
          }
        },
        "0deeecb2ef7d40029e962a71e23c0c96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6af81b961bf847558a4e2b17910b007b",
            "max": 435797,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fcb2b769d22e4191966a67c07a2edbe3",
            "value": 435797
          }
        },
        "c8d7ed7323a941028b07a47cb6f96378": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4821cb781a904317ac86f2d2ff90be68",
            "placeholder": "​",
            "style": "IPY_MODEL_01fa9562c1cd4137889b2efcca27e930",
            "value": " 436k/436k [00:00&lt;00:00, 4.50MB/s]"
          }
        },
        "4127176475e44814b9a4904af8571f44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6af81b961bf847558a4e2b17910b007b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcb2b769d22e4191966a67c07a2edbe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "4821cb781a904317ac86f2d2ff90be68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01fa9562c1cd4137889b2efcca27e930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f24d422358c42a299e0ffe27f4d7ef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0b59fbc2229d4139aa0b4b08d18a4df4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_92082de8a8ff4a4e8916d6ac8db5c155",
              "IPY_MODEL_6bf2823f7cf245588c51de8e23b7e2ca"
            ]
          }
        },
        "0b59fbc2229d4139aa0b4b08d18a4df4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "92082de8a8ff4a4e8916d6ac8db5c155": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5e08651d7c73447884c770bb814028fd",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 213450,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 213450,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_486e979978914094aa383a19a50452ac"
          }
        },
        "6bf2823f7cf245588c51de8e23b7e2ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7fd9a56c06354253815bcb06bbb403a8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 213k/213k [00:00&lt;00:00, 422kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_660483e917024b2f8a134a9225fbbb3e"
          }
        },
        "5e08651d7c73447884c770bb814028fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "486e979978914094aa383a19a50452ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7fd9a56c06354253815bcb06bbb403a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "660483e917024b2f8a134a9225fbbb3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "527eb39207264a0ba98840b0df6df190": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0e1aa6cd4d934810bb6f3ae29f746bd0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_03f0fe8902d049dd8aef36c160a81f17",
              "IPY_MODEL_9df449f372c545c2ab848291d05b74eb"
            ]
          }
        },
        "0e1aa6cd4d934810bb6f3ae29f746bd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "03f0fe8902d049dd8aef36c160a81f17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b8a2caa001814457aa91d330a5eaa0a5",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 29,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 29,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e5b2979a61914739a9f2d01efb0622c0"
          }
        },
        "9df449f372c545c2ab848291d05b74eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_486656af990c4da6a425612857fb71d1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 29.0/29.0 [00:00&lt;00:00, 118B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ec6baaafad4548b9b9f724fc34a0ff88"
          }
        },
        "b8a2caa001814457aa91d330a5eaa0a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e5b2979a61914739a9f2d01efb0622c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "486656af990c4da6a425612857fb71d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ec6baaafad4548b9b9f724fc34a0ff88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "850e864202ef423bb9bac5b44c4b934f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fca79f1d4cfe44dd89abd05175503f11",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_75a834a7836643a4bfa7f12de1ba0715",
              "IPY_MODEL_d7ac076c1dba429abb8c1fbf4d222c11"
            ]
          }
        },
        "fca79f1d4cfe44dd89abd05175503f11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "75a834a7836643a4bfa7f12de1ba0715": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_69323525b13d4f77a15086f7aad037cf",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 435797,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 435797,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6a0eae561f094785b0310f139b6c1981"
          }
        },
        "d7ac076c1dba429abb8c1fbf4d222c11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0a37eee897ce4c3a8c9748ed2a69c7d8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 436k/436k [00:00&lt;00:00, 3.99MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1362b0fc28c54fb6b8cbc4108664e3cd"
          }
        },
        "69323525b13d4f77a15086f7aad037cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6a0eae561f094785b0310f139b6c1981": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0a37eee897ce4c3a8c9748ed2a69c7d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1362b0fc28c54fb6b8cbc4108664e3cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3d04448d6bde496f9ea477525a8ab616": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_07590322348a456ca523c81e7283f1fd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3460799fca474d35afa4e4bee0cc7014",
              "IPY_MODEL_b4c895e40d984de4b0bad997e55a7ab1"
            ]
          }
        },
        "07590322348a456ca523c81e7283f1fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3460799fca474d35afa4e4bee0cc7014": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0d48fc8505944c68ba4fe43d4b5e4a5d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 442,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 442,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c7da1ad58efb479ba816051e461a99f1"
          }
        },
        "b4c895e40d984de4b0bad997e55a7ab1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6749edc2050b444f829b280b767c8760",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 442/442 [00:00&lt;00:00, 636B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b610044b0faa404ebca5ea9662b4e46d"
          }
        },
        "0d48fc8505944c68ba4fe43d4b5e4a5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c7da1ad58efb479ba816051e461a99f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6749edc2050b444f829b280b767c8760": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b610044b0faa404ebca5ea9662b4e46d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d68d78f82236477b8091a776ba484974": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3788ed5c74dc4471ba124cba43db9abf",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e9e25783156c4587a8c16b409dc3f73b",
              "IPY_MODEL_8dea1d72bbb547518be78bd31909ee83"
            ]
          }
        },
        "3788ed5c74dc4471ba124cba43db9abf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e9e25783156c4587a8c16b409dc3f73b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a9086eaadf024fb790e3a7b005e03fd3",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 267967963,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 267967963,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_63412876b3bd4f7eab577306ac5715d6"
          }
        },
        "8dea1d72bbb547518be78bd31909ee83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8a37afab66e64b2a9563444cc6ee8306",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 268M/268M [00:07&lt;00:00, 35.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e3d937374ea04ba196e6cee943d565c1"
          }
        },
        "a9086eaadf024fb790e3a7b005e03fd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "63412876b3bd4f7eab577306ac5715d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8a37afab66e64b2a9563444cc6ee8306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e3d937374ea04ba196e6cee943d565c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00cfaf08-a9f3-4917-b9a5-a78ceef260bb"
      },
      "source": [
        "# Predicting Job seniority level from job title and description"
      ],
      "id": "00cfaf08-a9f3-4917-b9a5-a78ceef260bb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab31e0df-8fab-43ad-a5a0-9b612f57cb7e",
        "outputId": "b6117eb2-673b-44a4-cc5d-02b1e2da3bc4"
      },
      "source": [
        "!pip install torchtext\n",
        "!pip install spacy\n",
        "!pip install nltk\n",
        "!pip install transformers\n"
      ],
      "id": "ab31e0df-8fab-43ad-a5a0-9b612f57cb7e",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.9.0+cu102)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.41.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext) (3.7.4.3)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.2.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.6.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 19.5MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 39.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 42.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaa6c283-9a9a-4320-9f56-ed64dddf2bd9"
      },
      "source": [
        "## Exploratory Analysis"
      ],
      "id": "aaa6c283-9a9a-4320-9f56-ed64dddf2bd9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e3d1de4-066f-4b3b-9cdc-f698d75113c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ea14252-de9e-4681-b8e0-e254250d333c"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "from xgboost import XGBClassifier"
      ],
      "id": "0e3d1de4-066f-4b3b-9cdc-f698d75113c8",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02f2cff7-7c42-4015-b281-34ee05ffdf89"
      },
      "source": [
        "def load_data():\n",
        "    return pd.read_json(\"./data.json\")"
      ],
      "id": "02f2cff7-7c42-4015-b281-34ee05ffdf89",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dea80bfe-d9bf-4ec5-bac8-14e6bbcd807a"
      },
      "source": [
        "data = load_data()"
      ],
      "id": "dea80bfe-d9bf-4ec5-bac8-14e6bbcd807a",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3v9PIWjOPMw",
        "outputId": "e19a2556-7898-4d98-ef45-46c895d8571f"
      },
      "source": [
        "len(data)"
      ],
      "id": "l3v9PIWjOPMw",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "216"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be5f33dd-9ed7-4aa4-b664-60117b4644b0"
      },
      "source": [
        "data[\"title_len\"] = data.title.apply(lambda text: len(text))\n",
        "data[\"description_len\"] = data.description.apply(lambda text: len(text))"
      ],
      "id": "be5f33dd-9ed7-4aa4-b664-60117b4644b0",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8134a8fe-363f-4ce5-af8c-d30c93fb4f7c",
        "outputId": "56d52e76-cee8-49bf-b649-865566fc50f1"
      },
      "source": [
        "print(np.max(data.title_len), np.mean(data.title_len), np.median(data.title_len))\n",
        "print(np.max(data.description_len), np.mean(data.description_len), np.median(data.description_len))"
      ],
      "id": "8134a8fe-363f-4ce5-af8c-d30c93fb4f7c",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "95 30.328703703703702 28.0\n",
            "5927 1764.8194444444443 1788.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-tiWqBZQw0x",
        "outputId": "ae42eef6-5e6a-476a-f94b-59bc177cf132"
      },
      "source": [
        "data.description_len.describe()"
      ],
      "id": "P-tiWqBZQw0x",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     216.000000\n",
              "mean     1764.819444\n",
              "std      1069.707932\n",
              "min         2.000000\n",
              "25%       973.000000\n",
              "50%      1788.000000\n",
              "75%      2493.250000\n",
              "max      5927.000000\n",
              "Name: description_len, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "c542f5b1-ed98-4026-b5fc-42199891b7a0",
        "outputId": "0513d82c-2a43-49f5-803d-989a150fdaf8"
      },
      "source": [
        "fig = plt.figure(figsize=(8,6))\n",
        "data.level.hist()\n",
        "plt.show()\n",
        "fig.savefig(\"./target_distribution.png\")"
      ],
      "id": "c542f5b1-ed98-4026-b5fc-42199891b7a0",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c6fb2b1-4de4-42d2-951e-808035bfa49a"
      },
      "source": [
        "def create_testset(data):\n",
        "    data_test = data.loc[data.level.isnull()]\n",
        "    data_train = data.loc[~data.level.isnull()]\n",
        "    return data_train, data_test"
      ],
      "id": "3c6fb2b1-4de4-42d2-951e-808035bfa49a",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15d8cfdb-1456-4f44-a7e1-1c1f3533e99e"
      },
      "source": [
        "df_train, df_test = create_testset(data)"
      ],
      "id": "15d8cfdb-1456-4f44-a7e1-1c1f3533e99e",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBSzkZ9SOSNq",
        "outputId": "a24220c8-9403-475d-fd93-3c35e2c8f092"
      },
      "source": [
        "df_test.description\n"
      ],
      "id": "KBSzkZ9SOSNq",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "141    OUTFITTERY is Europe’s biggest Personal Shoppi...\n",
              "142    OUTFITTERY is Europe’s biggest Personal Shoppi...\n",
              "143    OUTFITTERY is Europe’s biggest Personal Shoppi...\n",
              "144    OUTFITTERY is Europe’s biggest Personal Shoppi...\n",
              "145    OUTFITTERY is Europe’s biggest Personal Shoppi...\n",
              "                             ...                        \n",
              "211    Work in a one of a kind international environm...\n",
              "212    Work in a unique international environment and...\n",
              "213    Photography - New York Habitat Internship\\nWho...\n",
              "214    Are you free every day from 10:30am - 1:30pm a...\n",
              "215    OUTFITTERY is Europe’s biggest Personal Shoppi...\n",
              "Name: description, Length: 75, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79bb02fe-d71c-4729-bb09-307549f9b9a0"
      },
      "source": [
        "from sklearn.base import TransformerMixin\n",
        "\n",
        "class TFIDFTokenize(TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='utf-8', ngram_range=(1, 3), stop_words='english')\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.column_transform = ColumnTransformer(\n",
        "                                    [('tfidf_title', self.tfidf, 'title'),\n",
        "                                     ('tfidf_description', self.tfidf, 'description')])\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        X = self.column_transform.fit(X)\n",
        "        y = self.label_encoder.fit(y)\n",
        "        return X,y\n",
        "    def fit_transform(self, X, y):\n",
        "        X = self.column_transform.fit_transform(X)\n",
        "        y = self.label_encoder.fit_transform(y)\n",
        "        return X, y\n",
        "    def transform(self, X, y):\n",
        "        X = self.column_transform.transform(X)\n",
        "        y = self.label_encoder.transform(y)\n",
        "        return X, y\n"
      ],
      "id": "79bb02fe-d71c-4729-bb09-307549f9b9a0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3a83b61-5b1f-4dca-9558-352551dc3100"
      },
      "source": [
        "tf_idf_tokenize = TFIDFTokenize()\n",
        "X_train, y_train = tf_idf_tokenize.fit_transform(df_train[[\"title\", \"description\"]], df_train[\"level\"])"
      ],
      "id": "e3a83b61-5b1f-4dca-9558-352551dc3100",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b462f891-8c06-4794-93d2-dfbd5d20d805"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "def create_folds(X,y):\n",
        "    X = pd.DataFrame(X.toarray())\n",
        "    X[\"targets\"] = y\n",
        "    X[\"kfold\"] = -1\n",
        "    X = X.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X=X, y=X.targets.values)):\n",
        "        X.loc[val_idx, 'kfold'] = fold\n",
        "    \n",
        "    X.to_csv('data_folds.csv', index=False)"
      ],
      "id": "b462f891-8c06-4794-93d2-dfbd5d20d805",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ff96296-536b-446b-b400-66ae36c7eac4"
      },
      "source": [
        "def get_train_data(fold):\n",
        "    FOLD_MAPPPING = {\n",
        "        0: [1, 2, 3, 4],\n",
        "        1: [0, 2, 3, 4],\n",
        "        2: [0, 1, 3, 4],\n",
        "        3: [0, 1, 2, 4],\n",
        "        4: [0, 1, 2, 3]\n",
        "    }\n",
        "\n",
        "    df = pd.read_csv('data_folds.csv')\n",
        "    train_df = df[df.kfold.isin(FOLD_MAPPPING.get(fold))].reset_index(drop=True)\n",
        "    valid_df = df[df.kfold==fold].reset_index(drop=True)\n",
        "    ytrain = train_df.targets.values\n",
        "    yvalid = valid_df.targets.values\n",
        "    train_df = train_df.drop([\"targets\", \"kfold\"], axis=1)\n",
        "    valid_df = valid_df.drop([\"targets\", \"kfold\"],axis=1)\n",
        "    \n",
        "    \n",
        "    return (train_df, valid_df, ytrain, yvalid)"
      ],
      "id": "4ff96296-536b-446b-b400-66ae36c7eac4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ace0f729-7d12-46fd-8f0d-2447981fff74"
      },
      "source": [
        "def random_over_sampling(X, y):\n",
        "    ros = RandomOverSampler(random_state=42)\n",
        "\n",
        "    X_ros, y_ros = ros.fit_resample(X, y)\n",
        "    print(X_ros.shape[0] - X.shape[0], 'new random picked points')\n",
        "    print(sorted(Counter(y_ros).items()))\n",
        "    \n",
        "    return X_ros, y_ros"
      ],
      "id": "ace0f729-7d12-46fd-8f0d-2447981fff74",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e332ad0-54de-4a5d-91ad-e5e780505806"
      },
      "source": [
        "create_folds(X_train, y_train)"
      ],
      "id": "3e332ad0-54de-4a5d-91ad-e5e780505806",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55194732-66a3-403a-8084-c0b9f8eeef34"
      },
      "source": [
        "def train(model):\n",
        "    for fold in range(5):\n",
        "        train_df, valid_df, ytrain, yvalid = get_train_data(fold)\n",
        "        #train_df, ytrain = random_over_sampling(train_df, ytrain)\n",
        "\n",
        "        model.fit(train_df, ytrain)\n",
        "        y_pred = model.predict(valid_df)\n",
        "        print(metrics.accuracy_score(yvalid, y_pred))\n",
        "        print(\"Classification Report: \\n\", metrics.classification_report(yvalid, y_pred))\n",
        "        print(\"Confusion matrix: \\n\", metrics.confusion_matrix(yvalid, y_pred))"
      ],
      "id": "55194732-66a3-403a-8084-c0b9f8eeef34",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbfbb516-f9fb-45c0-a431-d9b630ca3d34",
        "outputId": "d92f7a6e-e064-42b5-c22b-eb2b01f5ca2e"
      },
      "source": [
        "model =  MultinomialNB()\n",
        "train(model)"
      ],
      "id": "fbfbb516-f9fb-45c0-a431-d9b630ca3d34",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5517241379310345\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.25      0.36         8\n",
            "           1       0.67      0.67      0.67         3\n",
            "           2       0.00      0.00      0.00         6\n",
            "           3       0.57      1.00      0.73        12\n",
            "\n",
            "    accuracy                           0.55        29\n",
            "   macro avg       0.48      0.48      0.44        29\n",
            "weighted avg       0.49      0.55      0.47        29\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 2  0  2  4]\n",
            " [ 0  2  0  1]\n",
            " [ 1  1  0  4]\n",
            " [ 0  0  0 12]]\n",
            "0.6071428571428571\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.43      0.50         7\n",
            "           1       0.60      1.00      0.75         3\n",
            "           2       1.00      0.17      0.29         6\n",
            "           3       0.59      0.83      0.69        12\n",
            "\n",
            "    accuracy                           0.61        28\n",
            "   macro avg       0.70      0.61      0.56        28\n",
            "weighted avg       0.68      0.61      0.56        28\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 3  1  0  3]\n",
            " [ 0  3  0  0]\n",
            " [ 1  0  1  4]\n",
            " [ 1  1  0 10]]\n",
            "0.4642857142857143\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.29      0.33         7\n",
            "           1       1.00      0.33      0.50         3\n",
            "           2       0.00      0.00      0.00         7\n",
            "           3       0.45      0.91      0.61        11\n",
            "\n",
            "    accuracy                           0.46        28\n",
            "   macro avg       0.46      0.38      0.36        28\n",
            "weighted avg       0.39      0.46      0.38        28\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 2  0  0  5]\n",
            " [ 1  1  0  1]\n",
            " [ 1  0  0  6]\n",
            " [ 1  0  0 10]]\n",
            "0.5714285714285714\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.43      0.55         7\n",
            "           1       1.00      0.67      0.80         3\n",
            "           2       0.00      0.00      0.00         7\n",
            "           3       0.52      1.00      0.69        11\n",
            "\n",
            "    accuracy                           0.57        28\n",
            "   macro avg       0.57      0.52      0.51        28\n",
            "weighted avg       0.50      0.57      0.49        28\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 3  0  1  3]\n",
            " [ 0  2  0  1]\n",
            " [ 1  0  0  6]\n",
            " [ 0  0  0 11]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.6428571428571429\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.50      0.53         8\n",
            "           1       1.00      1.00      1.00         3\n",
            "           2       0.00      0.00      0.00         6\n",
            "           3       0.65      1.00      0.79        11\n",
            "\n",
            "    accuracy                           0.64        28\n",
            "   macro avg       0.55      0.62      0.58        28\n",
            "weighted avg       0.52      0.64      0.57        28\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 4  0  1  3]\n",
            " [ 0  3  0  0]\n",
            " [ 3  0  0  3]\n",
            " [ 0  0  0 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87b11a20-77d7-4449-be0d-613624058e80",
        "outputId": "0ba7de2a-5495-4600-d84e-44157661d80f"
      },
      "source": [
        "tf_idf_tokenize.label_encoder.classes_"
      ],
      "id": "87b11a20-77d7-4449-be0d-613624058e80",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Entry Level', 'Internship', 'Mid Level', 'Senior Level'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a942f2be-da30-461a-9ce4-9758230439b4"
      },
      "source": [
        "Class `Mid Level` and `Internship` are under represented, we have to tp over sample. Even after the over sampling precession and accuracy for these classes are not better. In general, model is not doing better. I'll try with other models."
      ],
      "id": "a942f2be-da30-461a-9ce4-9758230439b4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7e9e628-6cde-4add-830d-5800298526fd",
        "outputId": "aa85a1e1-4b2a-4e94-a04c-2d96a8d75c96"
      },
      "source": [
        "model = XGBClassifier(objective='multi:softprob')\n",
        "train(model)"
      ],
      "id": "b7e9e628-6cde-4add-830d-5800298526fd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5172413793103449\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.38      0.43         8\n",
            "           1       0.60      1.00      0.75         3\n",
            "           2       0.17      0.17      0.17         6\n",
            "           3       0.67      0.67      0.67        12\n",
            "\n",
            "    accuracy                           0.52        29\n",
            "   macro avg       0.48      0.55      0.50        29\n",
            "weighted avg       0.51      0.52      0.51        29\n",
            "\n",
            "Confusion matrix: \n",
            " [[3 1 2 2]\n",
            " [0 3 0 0]\n",
            " [2 1 1 2]\n",
            " [1 0 3 8]]\n",
            "0.6785714285714286\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.57      0.53         7\n",
            "           1       1.00      0.33      0.50         3\n",
            "           2       0.50      0.50      0.50         6\n",
            "           3       0.85      0.92      0.88        12\n",
            "\n",
            "    accuracy                           0.68        28\n",
            "   macro avg       0.71      0.58      0.60        28\n",
            "weighted avg       0.70      0.68      0.67        28\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 4  0  2  1]\n",
            " [ 2  1  0  0]\n",
            " [ 2  0  3  1]\n",
            " [ 0  0  1 11]]\n",
            "0.6428571428571429\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.43      0.50         7\n",
            "           1       1.00      1.00      1.00         3\n",
            "           2       0.33      0.14      0.20         7\n",
            "           3       0.65      1.00      0.79        11\n",
            "\n",
            "    accuracy                           0.64        28\n",
            "   macro avg       0.65      0.64      0.62        28\n",
            "weighted avg       0.59      0.64      0.59        28\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 3  0  2  2]\n",
            " [ 0  3  0  0]\n",
            " [ 2  0  1  4]\n",
            " [ 0  0  0 11]]\n",
            "0.5714285714285714\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.43      0.55         7\n",
            "           1       1.00      1.00      1.00         3\n",
            "           2       0.29      0.29      0.29         7\n",
            "           3       0.57      0.73      0.64        11\n",
            "\n",
            "    accuracy                           0.57        28\n",
            "   macro avg       0.65      0.61      0.62        28\n",
            "weighted avg       0.59      0.57      0.57        28\n",
            "\n",
            "Confusion matrix: \n",
            " [[3 0 2 2]\n",
            " [0 3 0 0]\n",
            " [1 0 2 4]\n",
            " [0 0 3 8]]\n",
            "0.6785714285714286\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.88      0.67         8\n",
            "           1       1.00      1.00      1.00         3\n",
            "           2       0.00      0.00      0.00         6\n",
            "           3       0.90      0.82      0.86        11\n",
            "\n",
            "    accuracy                           0.68        28\n",
            "   macro avg       0.61      0.67      0.63        28\n",
            "weighted avg       0.61      0.68      0.63        28\n",
            "\n",
            "Confusion matrix: \n",
            " [[7 0 1 0]\n",
            " [0 3 0 0]\n",
            " [5 0 0 1]\n",
            " [1 0 1 9]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11d69aa2-4acb-4cbd-9656-9d50eeb078cc"
      },
      "source": [
        "## Using Embeding"
      ],
      "id": "11d69aa2-4acb-4cbd-9656-9d50eeb078cc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28149366-3b4f-491c-b54f-fb8dacb34b7d"
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext._torchtext import RegexTokenizer"
      ],
      "id": "28149366-3b4f-491c-b54f-fb8dacb34b7d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "4cb659cd-3aca-4f36-961f-3c794cccfa54",
        "outputId": "d146b870-bff3-4e90-dec0-a8161b0b3e9f"
      },
      "source": [
        "glove = torchtext.vocab.GloVe(name=\"6B\", # trained on Wikipedia 2014 corpus of 6 billion words\n",
        "                              dim=50)   # embedding size = 100"
      ],
      "id": "4cb659cd-3aca-4f36-961f-3c794cccfa54",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip:  19%|█▊        | 161M/862M [00:30<02:12, 5.29MB/s]  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-bd96711c185d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m glove = torchtext.vocab.GloVe(name=\"6B\", # trained on Wikipedia 2014 corpus of 6 billion words\n\u001b[0;32m----> 2\u001b[0;31m                               dim=50)   # embedding size = 100\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, dim, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'glove.{}.{}d.txt'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGloVe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, cache, url, unk_init, max_vectors)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munk_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0munk_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0munk_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36mcache\u001b[0;34m(self, name, cache, url, max_vectors)\u001b[0m\n\u001b[1;32m    360\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# remove the partial zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m                             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Extracting vectors into {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36mcache\u001b[0;34m(self, name, cache, url, max_vectors)\u001b[0m\n\u001b[1;32m    357\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'B'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminiters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                             \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreporthook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# remove the partial zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0fb1151-bee7-4718-bca2-18fb64f505f4"
      },
      "source": [
        "class MeanEmbeddingTransformer(TransformerMixin):\n",
        "    \n",
        "    def __init__(self):\n",
        "        self._E = torchtext.vocab.GloVe(name=\"6B\", # trained on Wikipedia 2014 corpus of 6 billion words\n",
        "                              dim=50)   # embedding size = 100\n",
        "    def _doc_mean(self, doc):\n",
        "        return np.mean(np.array([self._E[w].numpy() for w in doc]), axis=0)\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        return np.array([self._doc_mean(doc) for doc in X])\n",
        "    \n",
        "    def fit_transform(self, X, y=None):\n",
        "        return self.fit(X).transform(X)"
      ],
      "id": "a0fb1151-bee7-4718-bca2-18fb64f505f4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f61f9ec4-a74a-4a7f-9c02-cdd41e1fcf09"
      },
      "source": [
        "from spacy.lang.en import English\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import re\n",
        "\n",
        "class TextPreprocess:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def _tokenize(self, text):\n",
        "\n",
        "        nlp = English()\n",
        "        tokenizer = nlp.tokenizer\n",
        "        tokens = tokenizer(text)\n",
        "        tokens_list =   []\n",
        "        for token in tokens:\n",
        "            if (not token.is_stop) and \\\n",
        "            (not token.is_punct) and \\\n",
        "            (not token.is_digit) and \\\n",
        "            (len(token) > 3):\n",
        "                tokens_list.append(token.lower_)\n",
        "        return tokens_list\n",
        "\n",
        "    def _punctuation(self, tokens):\n",
        "        if isinstance((tokens), (str)):\n",
        "            tokens = re.sub('<[^>]*>', '', tokens)\n",
        "            tokens = re.sub('[\\W]+', '', tokens.lower())\n",
        "            return tokens\n",
        "        if isinstance((tokens), (list)):\n",
        "            return_list = []\n",
        "            for i in range(len(tokens)):\n",
        "                temp_text = re.sub('<[^>]*>', '', tokens[i])\n",
        "                temp_text = re.sub('[\\W]+', '', temp_text.lower())\n",
        "                return_list.append(temp_text)\n",
        "            return(return_list)\n",
        "        else:\n",
        "            pass\n",
        "    \n",
        "    def _pipelinize(self, function, active=True):\n",
        "        def list_comprehend_a_function(list_or_series, active=True):\n",
        "            if active:\n",
        "                return [function(i) for i in list_or_series]\n",
        "            else: # if it's not active, just pass it right back\n",
        "                return list_or_series\n",
        "        return FunctionTransformer(list_comprehend_a_function, validate=False, kw_args={'active':active})\n",
        "    \n",
        "    def __call__(self, text):\n",
        "        estimators = [('tokenizer', self._pipelinize(self._tokenize)),\n",
        "                      ('puntuation', self._pipelinize(self._punctuation))]\n",
        "        pipe = Pipeline(estimators)\n",
        "        return pipe.transform([text])[0]\n",
        "        "
      ],
      "id": "f61f9ec4-a74a-4a7f-9c02-cdd41e1fcf09",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41251e70-fda2-4185-a58e-e6320a0d0359"
      },
      "source": [
        "def tokenize_and_transform(X):\n",
        "    text_process = TextPreprocess()\n",
        "\n",
        "    X = X[['title', 'description']].values\n",
        "    title = X[:, 0]\n",
        "    #print(title)\n",
        "    description = X[:, 1]\n",
        "    #print(X)\n",
        "    title = [text_process(doc) for doc in title]\n",
        "    description = [text_process(doc) for doc in description]\n",
        "    met = MeanEmbeddingTransformer()\n",
        "    X_transform = np.concatenate([met.fit_transform(title), met.fit_transform(description)], axis=1)\n",
        "    return X_transform"
      ],
      "id": "41251e70-fda2-4185-a58e-e6320a0d0359",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3b33ad9-7602-4035-bb55-1836aec6fa0b"
      },
      "source": [
        "X_transform = tokenize_and_transform(df_train)"
      ],
      "id": "d3b33ad9-7602-4035-bb55-1836aec6fa0b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1186423f-6632-4980-96e5-6440f095f3d1"
      },
      "source": [
        "def train_with_embedings(model):\n",
        "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X=df_train, y=df_train.level.values)):\n",
        "        X_train = tokenize_and_transform(df_train.loc[train_idx, ['title', 'description']])\n",
        "        X_valid = tokenize_and_transform(df_train.loc[val_idx, ['title', 'description']])\n",
        "        y_train = df_train.loc[train_idx, 'level']\n",
        "        y_valid = df_train.loc[val_idx, 'level']\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_valid)\n",
        "        print(metrics.accuracy_score(y_valid, y_pred))\n",
        "        print(\"Classification Report: \\n\", metrics.classification_report(y_valid, y_pred))\n",
        "        print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_valid, y_pred))"
      ],
      "id": "1186423f-6632-4980-96e5-6440f095f3d1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47c31316-ecb8-483b-8fa7-fb6d4c946574"
      },
      "source": [
        "model = XGBClassifier(objective='multi:softprob')\n",
        "train_with_embedings(model)"
      ],
      "id": "47c31316-ecb8-483b-8fa7-fb6d4c946574",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78932cca-cb33-41bc-9283-32e8b75cded8"
      },
      "source": [
        "## Using Pretraid BERT model and pretrain for our task"
      ],
      "id": "78932cca-cb33-41bc-9283-32e8b75cded8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bfcc12d-1e16-48ae-a1e8-a063fa905bc9"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "id": "0bfcc12d-1e16-48ae-a1e8-a063fa905bc9",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164,
          "referenced_widgets": [
            "6db40fba6bf34537a6cc9fb95214cf1c",
            "022b7935ced14edb93a161192764dfef",
            "9b0f00e0faf54b598c0306661db50463",
            "d929ce6f0eb242a1b79ac1960621aa26",
            "62235fb123a14f7daeb21909fbbc9889",
            "19b586901676423db50fce5769bac21b",
            "27e949fae5d5431f852d6abfa4c47665",
            "15591c42effd499a88af3c17cbdd0402",
            "456de311b2db4a15abfd7f7e104bc844",
            "7a19733716464e969b2deb31f3cdc6f7",
            "0322df96ffe04f50a01d09f112941bca",
            "e85da681564c454c908132a1f99dcf50",
            "89d4242fc76d4531bc47bd258f436ba1",
            "b395c3104f634c9b8a5144dd38ed58bc",
            "5f60f6e1427444f7aef63b8b36878149",
            "270d32dd22794e71b9192257dc7744e9",
            "0290aeb843704674a555176f05590bf1",
            "0deeecb2ef7d40029e962a71e23c0c96",
            "c8d7ed7323a941028b07a47cb6f96378",
            "4127176475e44814b9a4904af8571f44",
            "6af81b961bf847558a4e2b17910b007b",
            "fcb2b769d22e4191966a67c07a2edbe3",
            "4821cb781a904317ac86f2d2ff90be68",
            "01fa9562c1cd4137889b2efcca27e930"
          ]
        },
        "id": "fa410042-e5ff-4ad6-9022-9ed166d31d9c",
        "outputId": "25f66817-9215-4023-95bb-f554645f9dc8"
      },
      "source": [
        "t = DistilBertTokenizer.from_pretrained('distilbert-base-cased')"
      ],
      "id": "fa410042-e5ff-4ad6-9022-9ed166d31d9c",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6db40fba6bf34537a6cc9fb95214cf1c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "456de311b2db4a15abfd7f7e104bc844",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=29.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0290aeb843704674a555176f05590bf1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435797.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fafef688-5df7-4323-a2ab-8154bc17bf19"
      },
      "source": [
        "class JobsDataset(Dataset):\n",
        "    def __init__(self, dataframe, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
        "        self.max_len = max_len\n",
        "        self._label_encode()\n",
        "\n",
        "    \n",
        "    def _label_encode(self):\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.data[\"level\"] = self.label_encoder.fit_transform(self.data.level)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        title = str(self.data.title[index])\n",
        "        title = \" \".join(title.split())\n",
        "        description = str(self.data.description[index])\n",
        "        description = \" \".join(description.split())\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text=title,\n",
        "            text_pair=description,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.data.level[index], dtype=torch.long)\n",
        "        } \n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "id": "fafef688-5df7-4323-a2ab-8154bc17bf19",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266,
          "referenced_widgets": [
            "0f24d422358c42a299e0ffe27f4d7ef5",
            "0b59fbc2229d4139aa0b4b08d18a4df4",
            "92082de8a8ff4a4e8916d6ac8db5c155",
            "6bf2823f7cf245588c51de8e23b7e2ca",
            "5e08651d7c73447884c770bb814028fd",
            "486e979978914094aa383a19a50452ac",
            "7fd9a56c06354253815bcb06bbb403a8",
            "660483e917024b2f8a134a9225fbbb3e",
            "527eb39207264a0ba98840b0df6df190",
            "0e1aa6cd4d934810bb6f3ae29f746bd0",
            "03f0fe8902d049dd8aef36c160a81f17",
            "9df449f372c545c2ab848291d05b74eb",
            "b8a2caa001814457aa91d330a5eaa0a5",
            "e5b2979a61914739a9f2d01efb0622c0",
            "486656af990c4da6a425612857fb71d1",
            "ec6baaafad4548b9b9f724fc34a0ff88",
            "850e864202ef423bb9bac5b44c4b934f",
            "fca79f1d4cfe44dd89abd05175503f11",
            "75a834a7836643a4bfa7f12de1ba0715",
            "d7ac076c1dba429abb8c1fbf4d222c11",
            "69323525b13d4f77a15086f7aad037cf",
            "6a0eae561f094785b0310f139b6c1981",
            "0a37eee897ce4c3a8c9748ed2a69c7d8",
            "1362b0fc28c54fb6b8cbc4108664e3cd"
          ]
        },
        "id": "85a30ee6-01e4-4772-983b-5c189b4037ec",
        "outputId": "ef5eba20-ec6b-4ce5-b5d4-bf51afc2a0fa"
      },
      "source": [
        "# Creating the dataset and dataloader for the neural network\n",
        "\n",
        "training_set = JobsDataset(df_train, 512)\n",
        "validation_set = JobsDataset(df_train, 512)"
      ],
      "id": "85a30ee6-01e4-4772-983b-5c189b4037ec",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f24d422358c42a299e0ffe27f4d7ef5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "527eb39207264a0ba98840b0df6df190",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=29.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "850e864202ef423bb9bac5b44c4b934f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435797.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "797d44fb-a020-4fed-9e02-79c68efacb27",
        "outputId": "9f5453c5-7992-4575-9a67-d865973d9e7b"
      },
      "source": [
        "train_params = {'batch_size': 16,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 4\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': 16,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 4\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(validation_set, **test_params)"
      ],
      "id": "797d44fb-a020-4fed-9e02-79c68efacb27",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56da2e2b-74d9-4503-8361-b7fca7d495d1"
      },
      "source": [
        "class DistillBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DistillBERTClass, self).__init__()\n",
        "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.classifier = torch.nn.Linear(768, 4)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output"
      ],
      "id": "56da2e2b-74d9-4503-8361-b7fca7d495d1",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186,
          "referenced_widgets": [
            "3d04448d6bde496f9ea477525a8ab616",
            "07590322348a456ca523c81e7283f1fd",
            "3460799fca474d35afa4e4bee0cc7014",
            "b4c895e40d984de4b0bad997e55a7ab1",
            "0d48fc8505944c68ba4fe43d4b5e4a5d",
            "c7da1ad58efb479ba816051e461a99f1",
            "6749edc2050b444f829b280b767c8760",
            "b610044b0faa404ebca5ea9662b4e46d",
            "d68d78f82236477b8091a776ba484974",
            "3788ed5c74dc4471ba124cba43db9abf",
            "e9e25783156c4587a8c16b409dc3f73b",
            "8dea1d72bbb547518be78bd31909ee83",
            "a9086eaadf024fb790e3a7b005e03fd3",
            "63412876b3bd4f7eab577306ac5715d6",
            "8a37afab66e64b2a9563444cc6ee8306",
            "e3d937374ea04ba196e6cee943d565c1"
          ]
        },
        "id": "e5ab923e-b64f-4f66-9f5c-ed1b9781acad",
        "outputId": "1c9695ba-4d8c-4893-d8ef-affe8b8fdeec"
      },
      "source": [
        "model = DistillBERTClass()\n",
        "model = model.to(device)"
      ],
      "id": "e5ab923e-b64f-4f66-9f5c-ed1b9781acad",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d04448d6bde496f9ea477525a8ab616",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d68d78f82236477b8091a776ba484974",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267967963.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54d58673-2f7f-4c0a-b5b0-165aadcba74e"
      },
      "source": [
        "LEARNING_RATE = 1e-03\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ],
      "id": "54d58673-2f7f-4c0a-b5b0-165aadcba74e",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "695eea6d-6449-40c0-bf42-3b3a66e3a933"
      },
      "source": [
        "def calcuate_accu(big_idx, targets):\n",
        "    n_correct = (big_idx==targets).sum().item()\n",
        "    return n_correct"
      ],
      "id": "695eea6d-6449-40c0-bf42-3b3a66e3a933",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "813b51de-bf48-42d5-a40a-bebcc127d490"
      },
      "source": [
        "def train(epoch):\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    y_preds = []\n",
        "    y_true = []\n",
        "    model.train()\n",
        "    for _,data in enumerate(training_loader, 0):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(ids, mask)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        tr_loss += loss.item()\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "        n_correct += calcuate_accu(big_idx, targets)\n",
        "        y_preds.extend(list(big_idx.cpu().detach().numpy()))\n",
        "        y_true.extend(list(targets.cpu().detach().numpy()))\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=targets.size(0)\n",
        "        \n",
        "        if _%5000==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            accu_step = (n_correct*100)/nb_tr_examples \n",
        "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
        "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # # When using GPU\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
        "    #print(metrics.accuracy_score(y_true, y_preds))\n",
        "    print(f\"Precision: {metrics.precision_score(y_true, y_preds, average='weighted')}\")\n",
        "    print(f\"Accuracy: {metrics.accuracy_score(y_true, y_preds)}\")\n",
        "    print(f\"Recall: {metrics.recall_score(y_true, y_preds, average='weighted')}\")\n",
        "    print(f\"F1 score: {metrics.f1_score(y_true, y_preds, average='weighted')}\")\n",
        "    print(\"Classification Report: \\n\", metrics.classification_report(y_true, y_preds))\n",
        "    print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_true, y_preds))\n",
        "\n",
        "    return "
      ],
      "id": "813b51de-bf48-42d5-a40a-bebcc127d490",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd73e625-0962-4577-97bd-96271d5589ac",
        "outputId": "a373c8bf-de05-43b8-b63d-2a4abe92d313"
      },
      "source": [
        "for epoch in range(50):\n",
        "    train(epoch)"
      ],
      "id": "cd73e625-0962-4577-97bd-96271d5589ac",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.4919551610946655\n",
            "Training Accuracy per 5000 steps: 12.5\n",
            "The Total Accuracy for Epoch 0: 29.78723404255319\n",
            "Training Loss Epoch: 1.4064602189593844\n",
            "Training Accuracy Epoch: 29.78723404255319\n",
            "Precision: 0.20484633569739952\n",
            "Accuracy: 0.2978723404255319\n",
            "Recall: 0.2978723404255319\n",
            "F1 score: 0.2329727148689448\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.15      0.08      0.11        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.05      0.03      0.04        32\n",
            "           3       0.38      0.67      0.49        57\n",
            "\n",
            "    accuracy                           0.30       141\n",
            "   macro avg       0.14      0.19      0.16       141\n",
            "weighted avg       0.20      0.30      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 3  0  9 25]\n",
            " [ 1  0  4 10]\n",
            " [ 5  0  1 26]\n",
            " [11  0  8 38]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.5007097721099854\n",
            "Training Accuracy per 5000 steps: 18.75\n",
            "The Total Accuracy for Epoch 1: 36.170212765957444\n",
            "Training Loss Epoch: 1.3562612003750272\n",
            "Training Accuracy Epoch: 36.170212765957444\n",
            "Precision: 0.32964549748100963\n",
            "Accuracy: 0.3617021276595745\n",
            "Recall: 0.3617021276595745\n",
            "F1 score: 0.3356363031202482\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.22      0.27        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.28      0.47      0.35        32\n",
            "           3       0.43      0.49      0.46        57\n",
            "\n",
            "    accuracy                           0.36       141\n",
            "   macro avg       0.27      0.29      0.27       141\n",
            "weighted avg       0.33      0.36      0.34       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 8  0 15 14]\n",
            " [ 1  0  3 11]\n",
            " [ 5  0 15 12]\n",
            " [ 9  0 20 28]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.4252909421920776\n",
            "Training Accuracy per 5000 steps: 31.25\n",
            "The Total Accuracy for Epoch 2: 40.42553191489362\n",
            "Training Loss Epoch: 1.3248252868652344\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.25151608592866687\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.24530823786142938\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.03      0.05        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.41      0.98      0.57        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.18      0.25      0.16       141\n",
            "weighted avg       0.25      0.40      0.25       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 1  0  0 36]\n",
            " [ 0  0  0 15]\n",
            " [ 1  0  0 31]\n",
            " [ 1  0  0 56]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.2217386960983276\n",
            "Training Accuracy per 5000 steps: 31.25\n",
            "The Total Accuracy for Epoch 3: 30.49645390070922\n",
            "Training Loss Epoch: 1.35323166847229\n",
            "Training Accuracy Epoch: 30.49645390070922\n",
            "Precision: 0.18662196179195517\n",
            "Accuracy: 0.3049645390070922\n",
            "Recall: 0.3049645390070922\n",
            "F1 score: 0.2310553161616991\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.14      0.19      0.16        32\n",
            "           3       0.38      0.65      0.48        57\n",
            "\n",
            "    accuracy                           0.30       141\n",
            "   macro avg       0.13      0.21      0.16       141\n",
            "weighted avg       0.19      0.30      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0 11 26]\n",
            " [ 0  0  6  9]\n",
            " [ 1  0  6 25]\n",
            " [ 1  0 19 37]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.4324698448181152\n",
            "Training Accuracy per 5000 steps: 31.25\n",
            "The Total Accuracy for Epoch 4: 26.24113475177305\n",
            "Training Loss Epoch: 1.3456386857562594\n",
            "Training Accuracy Epoch: 26.24113475177305\n",
            "Precision: 0.18247473566622505\n",
            "Accuracy: 0.2624113475177305\n",
            "Recall: 0.2624113475177305\n",
            "F1 score: 0.21016034535923528\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.23      0.49      0.31        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.30      0.33      0.32        57\n",
            "\n",
            "    accuracy                           0.26       141\n",
            "   macro avg       0.13      0.20      0.16       141\n",
            "weighted avg       0.18      0.26      0.21       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[18  0  0 19]\n",
            " [10  0  0  5]\n",
            " [12  0  0 20]\n",
            " [38  0  0 19]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.3113377094268799\n",
            "Training Accuracy per 5000 steps: 37.5\n",
            "The Total Accuracy for Epoch 5: 40.42553191489362\n",
            "Training Loss Epoch: 1.3563573625352647\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.2477084398269653\n",
            "Training Accuracy per 5000 steps: 43.75\n",
            "The Total Accuracy for Epoch 6: 41.843971631205676\n",
            "Training Loss Epoch: 1.3033739063474867\n",
            "Training Accuracy Epoch: 41.843971631205676\n",
            "Precision: 0.27856121831876407\n",
            "Accuracy: 0.41843971631205673\n",
            "Recall: 0.41843971631205673\n",
            "F1 score: 0.2882822312178132\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.14      0.20        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.42      0.95      0.58        57\n",
            "\n",
            "    accuracy                           0.42       141\n",
            "   macro avg       0.21      0.27      0.20       141\n",
            "weighted avg       0.28      0.42      0.29       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 5  0  0 32]\n",
            " [ 0  0  0 15]\n",
            " [ 4  0  0 28]\n",
            " [ 3  0  0 54]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.2499513626098633\n",
            "Training Accuracy per 5000 steps: 43.75\n",
            "The Total Accuracy for Epoch 7: 40.42553191489362\n",
            "Training Loss Epoch: 1.3312690787845187\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.2934979551690221\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.25481804061295715\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.05      0.10        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      0.96      0.57        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.23      0.25      0.17       141\n",
            "weighted avg       0.29      0.40      0.25       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 2  0  0 35]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 2  0  0 55]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.352394700050354\n",
            "Training Accuracy per 5000 steps: 37.5\n",
            "The Total Accuracy for Epoch 8: 39.00709219858156\n",
            "Training Loss Epoch: 1.3170448674096003\n",
            "Training Accuracy Epoch: 39.00709219858156\n",
            "Precision: 0.32312814682475227\n",
            "Accuracy: 0.3900709219858156\n",
            "Recall: 0.3900709219858156\n",
            "F1 score: 0.2527357004948676\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.03      0.05        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.12      0.03      0.05        32\n",
            "           3       0.40      0.93      0.56        57\n",
            "\n",
            "    accuracy                           0.39       141\n",
            "   macro avg       0.26      0.25      0.17       141\n",
            "weighted avg       0.32      0.39      0.25       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 1  0  3 33]\n",
            " [ 0  0  1 14]\n",
            " [ 0  0  1 31]\n",
            " [ 1  0  3 53]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.227081537246704\n",
            "Training Accuracy per 5000 steps: 50.0\n",
            "The Total Accuracy for Epoch 9: 39.00709219858156\n",
            "Training Loss Epoch: 1.3029898007710774\n",
            "Training Accuracy Epoch: 39.00709219858156\n",
            "Precision: 0.23486669993497555\n",
            "Accuracy: 0.3900709219858156\n",
            "Recall: 0.3900709219858156\n",
            "F1 score: 0.24820669657474825\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.05      0.09        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      0.93      0.55        57\n",
            "\n",
            "    accuracy                           0.39       141\n",
            "   macro avg       0.17      0.25      0.16       141\n",
            "weighted avg       0.23      0.39      0.25       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 2  0  0 35]\n",
            " [ 0  0  0 15]\n",
            " [ 1  0  0 31]\n",
            " [ 4  0  0 53]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.3402705192565918\n",
            "Training Accuracy per 5000 steps: 43.75\n",
            "The Total Accuracy for Epoch 10: 39.716312056737586\n",
            "Training Loss Epoch: 1.3094133933385212\n",
            "Training Accuracy Epoch: 39.716312056737586\n",
            "Precision: 0.16286545231899585\n",
            "Accuracy: 0.3971631205673759\n",
            "Recall: 0.3971631205673759\n",
            "F1 score: 0.23100303951367784\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      0.98      0.57        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 1  0  0 14]\n",
            " [ 0  0  0 32]\n",
            " [ 1  0  0 56]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.3227530717849731\n",
            "Training Accuracy per 5000 steps: 37.5\n",
            "The Total Accuracy for Epoch 11: 40.42553191489362\n",
            "Training Loss Epoch: 1.3117891152699788\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.3981075286865234\n",
            "Training Accuracy per 5000 steps: 43.75\n",
            "The Total Accuracy for Epoch 12: 40.42553191489362\n",
            "Training Loss Epoch: 1.3040163649453058\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.1731297969818115\n",
            "Training Accuracy per 5000 steps: 37.5\n",
            "The Total Accuracy for Epoch 13: 39.716312056737586\n",
            "Training Loss Epoch: 1.3062285052405462\n",
            "Training Accuracy Epoch: 39.716312056737586\n",
            "Precision: 0.16286545231899585\n",
            "Accuracy: 0.3971631205673759\n",
            "Recall: 0.3971631205673759\n",
            "F1 score: 0.23100303951367784\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      0.98      0.57        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 1  0  0 14]\n",
            " [ 0  0  0 32]\n",
            " [ 1  0  0 56]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.2050377130508423\n",
            "Training Accuracy per 5000 steps: 62.5\n",
            "The Total Accuracy for Epoch 14: 40.42553191489362\n",
            "Training Loss Epoch: 1.2946655485365126\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.299790382385254\n",
            "Training Accuracy per 5000 steps: 43.75\n",
            "The Total Accuracy for Epoch 15: 40.42553191489362\n",
            "Training Loss Epoch: 1.3231763177447848\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.1767321825027466\n",
            "Training Accuracy per 5000 steps: 62.5\n",
            "The Total Accuracy for Epoch 16: 40.42553191489362\n",
            "Training Loss Epoch: 1.307920840051439\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.2045552730560303\n",
            "Training Accuracy per 5000 steps: 43.75\n",
            "The Total Accuracy for Epoch 17: 40.42553191489362\n",
            "Training Loss Epoch: 1.3080027103424072\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.2824816703796387\n",
            "Training Accuracy per 5000 steps: 37.5\n",
            "The Total Accuracy for Epoch 18: 41.13475177304964\n",
            "Training Loss Epoch: 1.3074522150887384\n",
            "Training Accuracy Epoch: 41.13475177304964\n",
            "Precision: 0.4270010131712259\n",
            "Accuracy: 0.41134751773049644\n",
            "Recall: 0.41134751773049644\n",
            "F1 score: 0.24774567372096945\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.03      0.05        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.41      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.41       141\n",
            "   macro avg       0.35      0.26      0.16       141\n",
            "weighted avg       0.43      0.41      0.25       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 1  0  0 36]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.288136601448059\n",
            "Training Accuracy per 5000 steps: 50.0\n",
            "The Total Accuracy for Epoch 19: 40.42553191489362\n",
            "Training Loss Epoch: 1.3005239168802898\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.3444360494613647\n",
            "Training Accuracy per 5000 steps: 31.25\n",
            "The Total Accuracy for Epoch 20: 40.42553191489362\n",
            "Training Loss Epoch: 1.3011667595969305\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.1753162145614624\n",
            "Training Accuracy per 5000 steps: 50.0\n",
            "The Total Accuracy for Epoch 21: 40.42553191489362\n",
            "Training Loss Epoch: 1.3185833030276828\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.4236981868743896\n",
            "Training Accuracy per 5000 steps: 37.5\n",
            "The Total Accuracy for Epoch 22: 40.42553191489362\n",
            "Training Loss Epoch: 1.320298194885254\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.2811315059661865\n",
            "Training Accuracy per 5000 steps: 43.75\n",
            "The Total Accuracy for Epoch 23: 40.42553191489362\n",
            "Training Loss Epoch: 1.300109174516466\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.385535717010498\n",
            "Training Accuracy per 5000 steps: 31.25\n",
            "The Total Accuracy for Epoch 24: 40.42553191489362\n",
            "Training Loss Epoch: 1.313484960132175\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.439029574394226\n",
            "Training Accuracy per 5000 steps: 18.75\n",
            "The Total Accuracy for Epoch 25: 40.42553191489362\n",
            "Training Loss Epoch: 1.3070845471488104\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.2242047786712646\n",
            "Training Accuracy per 5000 steps: 50.0\n",
            "The Total Accuracy for Epoch 26: 40.42553191489362\n",
            "Training Loss Epoch: 1.2938222885131836\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.2383849620819092\n",
            "Training Accuracy per 5000 steps: 50.0\n",
            "The Total Accuracy for Epoch 27: 40.42553191489362\n",
            "Training Loss Epoch: 1.296237177318997\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.2722207307815552\n",
            "Training Accuracy per 5000 steps: 37.5\n",
            "The Total Accuracy for Epoch 28: 40.42553191489362\n",
            "Training Loss Epoch: 1.3008288277520075\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.5894516706466675\n",
            "Training Accuracy per 5000 steps: 25.0\n",
            "The Total Accuracy for Epoch 29: 40.42553191489362\n",
            "Training Loss Epoch: 1.3125920560624864\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.3453547954559326\n",
            "Training Accuracy per 5000 steps: 25.0\n",
            "The Total Accuracy for Epoch 30: 40.42553191489362\n",
            "Training Loss Epoch: 1.3068204058541193\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.4838203191757202\n",
            "Training Accuracy per 5000 steps: 31.25\n",
            "The Total Accuracy for Epoch 31: 40.42553191489362\n",
            "Training Loss Epoch: 1.3064734670850966\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.2590763568878174\n",
            "Training Accuracy per 5000 steps: 37.5\n",
            "The Total Accuracy for Epoch 32: 40.42553191489362\n",
            "Training Loss Epoch: 1.2975840436087713\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.2577077150344849\n",
            "Training Accuracy per 5000 steps: 37.5\n",
            "The Total Accuracy for Epoch 33: 41.13475177304964\n",
            "Training Loss Epoch: 1.3113491137822468\n",
            "Training Accuracy Epoch: 41.13475177304964\n",
            "Precision: 0.4270010131712259\n",
            "Accuracy: 0.41134751773049644\n",
            "Recall: 0.41134751773049644\n",
            "F1 score: 0.24774567372096945\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.03      0.05        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.41      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.41       141\n",
            "   macro avg       0.35      0.26      0.16       141\n",
            "weighted avg       0.43      0.41      0.25       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 1  0  0 36]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.2890634536743164\n",
            "Training Accuracy per 5000 steps: 50.0\n",
            "The Total Accuracy for Epoch 34: 40.42553191489362\n",
            "Training Loss Epoch: 1.292551702923245\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.345952033996582\n",
            "Training Accuracy per 5000 steps: 31.25\n",
            "The Total Accuracy for Epoch 35: 40.42553191489362\n",
            "Training Loss Epoch: 1.3047138452529907\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.1162396669387817\n",
            "Training Accuracy per 5000 steps: 50.0\n",
            "The Total Accuracy for Epoch 36: 40.42553191489362\n",
            "Training Loss Epoch: 1.2983293930689495\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.478130578994751\n",
            "Training Accuracy per 5000 steps: 25.0\n",
            "The Total Accuracy for Epoch 37: 40.42553191489362\n",
            "Training Loss Epoch: 1.315945757759942\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.5413058996200562\n",
            "Training Accuracy per 5000 steps: 18.75\n",
            "The Total Accuracy for Epoch 38: 40.42553191489362\n",
            "Training Loss Epoch: 1.2996850146187677\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.3093764781951904\n",
            "Training Accuracy per 5000 steps: 37.5\n",
            "The Total Accuracy for Epoch 39: 40.42553191489362\n",
            "Training Loss Epoch: 1.2931603988011677\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.1398588418960571\n",
            "Training Accuracy per 5000 steps: 56.25\n",
            "The Total Accuracy for Epoch 40: 40.42553191489362\n",
            "Training Loss Epoch: 1.3250541422102187\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.2275512218475342\n",
            "Training Accuracy per 5000 steps: 50.0\n",
            "The Total Accuracy for Epoch 41: 40.42553191489362\n",
            "Training Loss Epoch: 1.307007246547275\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.419242024421692\n",
            "Training Accuracy per 5000 steps: 37.5\n",
            "The Total Accuracy for Epoch 42: 40.42553191489362\n",
            "Training Loss Epoch: 1.3090740177366469\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.2770406007766724\n",
            "Training Accuracy per 5000 steps: 31.25\n",
            "The Total Accuracy for Epoch 43: 40.42553191489362\n",
            "Training Loss Epoch: 1.331023375193278\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.2767760753631592\n",
            "Training Accuracy per 5000 steps: 31.25\n",
            "The Total Accuracy for Epoch 44: 40.42553191489362\n",
            "Training Loss Epoch: 1.311739484469096\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.2974516153335571\n",
            "Training Accuracy per 5000 steps: 50.0\n",
            "The Total Accuracy for Epoch 45: 40.42553191489362\n",
            "Training Loss Epoch: 1.2864548630184598\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.2179954051971436\n",
            "Training Accuracy per 5000 steps: 43.75\n",
            "The Total Accuracy for Epoch 46: 40.42553191489362\n",
            "Training Loss Epoch: 1.3184334172142878\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.2723469734191895\n",
            "Training Accuracy per 5000 steps: 68.75\n",
            "The Total Accuracy for Epoch 47: 40.42553191489362\n",
            "Training Loss Epoch: 1.3072560363345676\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.2388206720352173\n",
            "Training Accuracy per 5000 steps: 37.5\n",
            "The Total Accuracy for Epoch 48: 40.42553191489362\n",
            "Training Loss Epoch: 1.2995729049046834\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.2153315544128418\n",
            "Training Accuracy per 5000 steps: 50.0\n",
            "The Total Accuracy for Epoch 49: 40.42553191489362\n",
            "Training Loss Epoch: 1.3137777116563585\n",
            "Training Accuracy Epoch: 40.42553191489362\n",
            "Precision: 0.16342236306020824\n",
            "Accuracy: 0.40425531914893614\n",
            "Recall: 0.40425531914893614\n",
            "F1 score: 0.23275306254029654\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.00      0.00      0.00        15\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.40      1.00      0.58        57\n",
            "\n",
            "    accuracy                           0.40       141\n",
            "   macro avg       0.10      0.25      0.14       141\n",
            "weighted avg       0.16      0.40      0.23       141\n",
            "\n",
            "Confusion matrix: \n",
            " [[ 0  0  0 37]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0  0 32]\n",
            " [ 0  0  0 57]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWccM8AIeh9_",
        "outputId": "dfb3482b-e332-4571-dc9e-f74a3a74d4d8"
      },
      "source": [
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit_transform(df_train.level)"
      ],
      "id": "CWccM8AIeh9_",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 3, 0, 3, 0, 0, 0, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3,\n",
              "       3, 0, 3, 3, 0, 2, 0, 3, 3, 3, 1, 3, 0, 3, 2, 2, 3, 0, 3, 3, 3, 3,\n",
              "       2, 2, 0, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 2, 0, 2, 3, 1, 1, 2,\n",
              "       1, 1, 1, 0, 1, 1, 1, 2, 2, 3, 2, 2, 2, 0, 0, 0, 0, 3, 2, 3, 0, 0,\n",
              "       3, 0, 0, 3, 3, 0, 0, 0, 0, 0, 2, 0, 2, 2, 0, 2, 3, 2, 1, 3, 2, 3,\n",
              "       2, 0, 0, 3, 2, 1, 3, 0, 1, 3, 3, 0, 2, 3, 3, 1, 0, 0, 3, 3, 3, 3,\n",
              "       3, 2, 1, 1, 0, 0, 3, 0, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9zgb0t-epqu",
        "outputId": "8c27b2db-b2d0-42a2-8333-60b4c9758120"
      },
      "source": [
        "label_encoder.classes_\n"
      ],
      "id": "x9zgb0t-epqu",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Entry Level', 'Internship', 'Mid Level', 'Senior Level'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcez12XDfAuJ",
        "outputId": "6b9985ed-69a6-4312-a468-47f225c5b7fe"
      },
      "source": [
        "label_encoder.inverse_transform([0, 1, 2, 3])"
      ],
      "id": "mcez12XDfAuJ",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Entry Level', 'Internship', 'Mid Level', 'Senior Level'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    }
  ]
}